{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The first pip install is super important, make sure it is installed and you restart the kernel. **Make sure you import in the text files for the articles into your working director.**"
      ],
      "metadata": {
        "id": "UOR--dtSb0Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl\n"
      ],
      "metadata": {
        "id": "Dc3kumuwBbVW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we begin by importing the text files, we suggest you do sentiment analysis for each topic separately and create a CSV file after each sentiment analysis. Later we will show you how we create the CSV."
      ],
      "metadata": {
        "id": "1eWpZvZ4erZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This makes it easy to grab everything titled (blah blah blah)whateveryouwant.txt\n",
        "import glob\n",
        "file_list = glob.glob('*Israel.txt')"
      ],
      "metadata": {
        "id": "wZw6YZnJyhmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double check that the files imported properly\n",
        "print(file_list)"
      ],
      "metadata": {
        "id": "aFeP7yKvTsW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load in the articles and read them\n",
        "def load_article(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n"
      ],
      "metadata": {
        "id": "UuhRpgMqTLyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our main thing we are using to split up the text and allow it to be\n",
        "# used to create our model for sentiment analysis\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "def segment_article(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "6pjKMPdvBjhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_sentences(sentences, chunk_size=5):\n",
        "  \"\"\"A function designed to split up the text into chunks to\n",
        "  then input into the model\"\"\"\n",
        "  chunks = []\n",
        "  for i in range(0, len(sentences), chunk_size):\n",
        "      chunk = \" \".join(sentences[i:i+chunk_size])\n",
        "      chunks.append(chunk)\n",
        "  return chunks\n"
      ],
      "metadata": {
        "id": "9OTAc7sfDpVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Very important, importing from the hugging face library a pre-trained model\n",
        "# based off of twitter data that better understands context for sentiment\n",
        "# analysis\n",
        "from transformers import pipeline\n",
        "\n",
        "sentiment_model = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    return_all_scores=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "59CdUKeNR8aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_sentiment_output(output):\n",
        "    if isinstance(output, list) and isinstance(output[0], dict):\n",
        "        return output\n",
        "    if isinstance(output, dict):\n",
        "        return [output]\n",
        "    raise ValueError(f\"Unexpected sentiment output: {output}\")\n"
      ],
      "metadata": {
        "id": "gLvV-wOlUXeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discourse_weight(text):\n",
        "  \"\"\"Weighs some text slightly more compared to other words considering\n",
        "  their impact in the text\"\"\"\n",
        "  text = text.lower()\n",
        "  if \"in conclusion\" in text or \"overall\" in text:\n",
        "      return 1.5\n",
        "  if \"however\" in text or \"but\" in text:\n",
        "      return 1.3\n",
        "  return 1.0\n"
      ],
      "metadata": {
        "id": "AaIuFeawVkaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_chunks(chunks):\n",
        "  \"\"\"Analyzes the chunks individually to obtain a score for each chunk\"\"\"\n",
        "  results = []\n",
        "\n",
        "  for chunk in chunks:\n",
        "      raw_output = sentiment_model(chunk)\n",
        "      scores = normalize_sentiment_output(raw_output)\n",
        "\n",
        "      weight = discourse_weight(chunk)\n",
        "\n",
        "      weighted_scores = {}\n",
        "      for s in scores:\n",
        "          weighted_scores[s[\"label\"]] = s[\"score\"] * weight\n",
        "\n",
        "      results.append(weighted_scores)\n",
        "\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "vAxvF4xNSH4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def aggregate_sentiment(chunk_results):\n",
        "  \"\"\"Function to aggregate all chunk sentiment values to get a total document\n",
        "     wide understanding of the sentiment\"\"\"\n",
        "  totals = defaultdict(float)\n",
        "\n",
        "  for chunk in chunk_results:\n",
        "      for label, score in chunk.items():\n",
        "          totals[label] += score\n",
        "\n",
        "  total = sum(totals.values())\n",
        "  return {k: v / total for k, v in totals.items()}\n"
      ],
      "metadata": {
        "id": "1eqHUhefWNB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is putting everything together in a for loop to go over each document and obtain a sentiment value for each document. We then append it to a list, and convert it into a dictionary for easy conversion into a CSV file."
      ],
      "metadata": {
        "id": "svSa0xirbliE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "neutral = []\n",
        "negative = []"
      ],
      "metadata": {
        "id": "2PxSm-RoUI9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for txt in file_list:\n",
        "  article = load_article(txt)\n",
        "  sentences = segment_article(article)\n",
        "  chunks = chunk_sentences(sentences)\n",
        "  chunk_results = analyze_chunks(chunks)\n",
        "  doc_sentiment = aggregate_sentiment(chunk_results)\n",
        "  positive.append(doc_sentiment.get('positive', 0.0))\n",
        "  neutral.append(doc_sentiment.get('neutral', 0.0))\n",
        "  negative.append(doc_sentiment.get('negative', 0.0))\n",
        "\n",
        "# Checking that we have sentiment scores\n",
        "print(positive)\n",
        "print(neutral)\n",
        "print(negative)"
      ],
      "metadata": {
        "id": "RskN8NNwXs-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary_for_csv = {\n",
        "    'File Name': file_list,\n",
        "    'Positive': positive,\n",
        "    'Neutral': neutral,\n",
        "    'Negative': negative\n",
        "}"
      ],
      "metadata": {
        "id": "iX93Vy7t0WcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Converting into a CSV, rename to whatever data you are working with\n",
        "df = pd.DataFrame(dictionary_for_csv)\n",
        "df.to_csv('full_israel_sentiment.csv', index = False)"
      ],
      "metadata": {
        "id": "SIgpN_D472in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FRMVSeFT8CYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}